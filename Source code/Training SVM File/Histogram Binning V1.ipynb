{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Histogram Binning V1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPFXfRIx9NAYmHPr7Dqj6+5"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"20hH1yOOUZJp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"outputId":"311258f6-4080-4788-c816-82d53357ad7a","executionInfo":{"status":"ok","timestamp":1591274032735,"user_tz":-330,"elapsed":51447,"user":{"displayName":"HeeraMani Prasad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFiYzBBqP2izFsMN0lnUV4W2ntW4Icbk7de5sTkg=s64","userId":"09167360220955181140"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"F09Ni12_UbY_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":356},"outputId":"af1442a1-316e-42a7-ce31-77d40b4b8423","executionInfo":{"status":"ok","timestamp":1591282166382,"user_tz":-330,"elapsed":1308811,"user":{"displayName":"HeeraMani Prasad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFiYzBBqP2izFsMN0lnUV4W2ntW4Icbk7de5sTkg=s64","userId":"09167360220955181140"}}},"source":["#!/usr/bin/env python\n","# coding: utf-8\n","\n","# In[1]:\n","from skimage import feature\n","import numpy as np\n","import imutils\n","import cv2\n","import sys\n","import math\n","import glob\n","import os\n","import natsort\n","from pathlib import Path\n","import shutil\n","import time\n","import scipy.io\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn import svm\n","from sklearn import metrics\n","import pandas as pd\n","from matplotlib import pyplot as plt\n","import glob\n","\n","from numpy import unique\n","from numpy import hstack\n","from numpy import vstack\n","from numpy import where\n","from matplotlib import pyplot\n","from sklearn.datasets import make_blobs\n","\n","import xlwt\n","from xlwt import Workbook\n","import openpyxl\n","# %%\n","# NAMES = ['1_tatta', '2_natta', '4_kuditta_mettu', '5_kuditta_nattal',\\\n","#          '8_tei_tei_dhatta', \\\n","#    '9_katti_kartari','10_utsanga', '11_mandi',\\\n","#             '13_tirmana','14_sarika', '15_joining']\n","\n","NAMES = ['6_kuditta_tattal']\n","ALL_CSVFILE = '/content/drive/My Drive/Final Result V1/Final Result V1/Resized_Histogram_Binning/'\n","i = 1\n","f = open(os.path.join(ALL_CSVFILE, 'Results.csv'), 'a')\n","print('nomenclature',\n","      ',',\n","      'accuracy_mf_kf',\n","      ',',\n","      'accuracy_mf',\n","      ',',\n","      'accuracy_kf',\n","      ',',\n","      'precision_mf',\n","      ',',\n","      'precision_kf',\n","      ',',\n","      'recall_mf',\n","      ',',\n","      'recall_kf',\n","      ',',\n","      'f1_mf',\n","      ',',\n","      'f1_kf',\n","      file=f)\n","f.close()\n","\n","for NAME in NAMES:\n","    NAME_ = NAME + ''\n","    NAME2 = '_Histogram_Binning'\n","\n","    NAME3 = NAME_ + NAME2\n","\n","    PATH = os.path.join(\n","        '/content/drive/My Drive/Final Result V1/Final Result V1/Resized_Histogram_Binning/',\n","        NAME3)\n","    PATH = PATH + '/'\n","    \n","\n","    extracted_path = os.path.join(PATH ,  '*.mat')\n","    print(extracted_path)\n","\n","    SAVE_FILE = NAME + '.csv'\n","\n","    nomenclature = NAME\n","\n","    X_init = np.empty((1, 9))\n","    Y_init = np.empty((1, 1))\n","\n","    for filename in glob.glob(extracted_path):\n","        print(i, '\\t', filename)\n","        head, tail = os.path.split(filename)\n","        # print(tail)\n","\n","        mat = scipy.io.loadmat(filename)\n","\n","        X_data = mat['X_final']\n","        Y_data = mat['Y_final']\n","\n","        # print(X_data.shape, Y_data.shape)\n","\n","        ###     For all train\n","        X_init = np.vstack((X_init, X_data))\n","        Y_init = np.vstack((Y_init, Y_data))\n","#%%\n","##########delete first row and column\n","# print(X_init.shape)\n","# print(Y_init.shape)\n","    X_init = X_init[1:, :]\n","    Y_init = Y_init[1:, :]\n","\n","    X = np.nan_to_num(X_init)\n","    Y = Y_init.ravel()\n","\n","    print(X.shape)\n","    print(Y.shape)\n","\n","    #### Split dataset into training set and test set\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        X, Y, test_size=0.2,\n","        random_state=3)  # 70% training and 30% test\n","\n","    y_train_len = len(y_train)\n","    n_mf_train = np.count_nonzero(y_train)\n","    n_kf_train = y_train_len - n_mf_train\n","    percent_mf_train = 100 * (n_mf_train / y_train_len)\n","    percent_kf_train = 100 * (n_kf_train / y_train_len)\n","\n","    print(percent_mf_train, percent_kf_train)\n","    #########################################################################\n","    #########################################################################\n","    #########################################################################\n","    kernel = 'linear'\n","    #Create a svm Classifier\n","    clf = svm.SVC(kernel=kernel,\n","                  class_weight={\n","                      1: percent_kf_train / 10,\n","                      0: percent_mf_train / 10\n","                  })  # Linear Kernel\n","    # clf = svm.SVC(kernel=kernel)  # Linear Kernel\n","\n","    #Train the model using the training sets\n","    clf.fit(X_train, y_train)\n","\n","    #Predict the response for test dataset\n","    y_pred = clf.predict(X_test)\n","    # Print the confusion matrix\n","\n","    accuracy = (metrics.accuracy_score(y_test, y_pred)) * 100\n","\n","    cm = metrics.confusion_matrix(y_test, y_pred)\n","\n","    TN = cm[0][0]\n","    FN = cm[1][0]\n","    TP = cm[1][1]\n","    FP = cm[0][1]\n","\n","    accuracy_mf_kf = accuracy\n","    accuracy_mf = (TP / (TP + FN)) * 100\n","    accuracy_kf = (TN / (TN + FP)) * 100\n","\n","    precision_mf = (TP / (TP + FP)) * 100\n","    precision_kf = (TN / (TN + FN)) * 100\n","\n","    recall_mf = (TP / (TP + FN)) * 100\n","\n","    recall_mf = (TP / (TP + FN)) * 100\n","    recall_kf = (TN / (TN + FP)) * 100\n","\n","    f1_mf = 2 * precision_mf * recall_mf / (precision_mf + recall_mf)\n","    f1_kf = 2 * precision_kf * recall_kf / (precision_kf + recall_kf)\n","\n","    train_accuracy = (clf.score(X_train, y_train)) * 100\n","    test_accuracy = (clf.score(X_test, y_test)) * 100\n","\n","    # y_len = len(Y_init)\n","    # n_mf = np.count_nonzero(Y_init)\n","    # n_kf = y_len - n_mf\n","    # percent_mf = 100 * (n_mf / y_len)\n","    # percent_kf = 100 * (n_kf / y_len)\n","\n","    f = open(os.path.join(ALL_CSVFILE, 'Results.csv'), 'a')\n","    print(nomenclature,\n","          ',',\n","          \"{:.2f}\".format(accuracy_mf_kf),\n","          ',',\n","          \"{:.2f}\".format(accuracy_mf),\n","          ',',\n","          \"{:.2f}\".format(accuracy_kf),\n","          ',',\n","          \"{:.2f}\".format(precision_mf),\n","          ',',\n","          \"{:.2f}\".format(precision_kf),\n","          ',',\n","          \"{:.2f}\".format(recall_mf),\n","          ',',\n","          \"{:.2f}\".format(recall_kf),\n","          ',',\n","          \"{:.2f}\".format(f1_mf),\n","          ',',\n","          \"{:.2f}\".format(f1_kf),\n","          file=f)\n","    f.close()\n","\n","    print(\n","        nomenclature,\n","        ',',\n","        \"{:.2f}\".format(accuracy_mf_kf),\n","        ',',\n","        \"{:.2f}\".format(accuracy_mf),\n","        ',',\n","        \"{:.2f}\".format(accuracy_kf),\n","        ',',\n","        \"{:.2f}\".format(precision_mf),\n","        ',',\n","        \"{:.2f}\".format(precision_kf),\n","        ',',\n","        \"{:.2f}\".format(recall_mf),\n","        ',',\n","        \"{:.2f}\".format(recall_kf),\n","        ',',\n","        \"{:.2f}\".format(f1_mf),\n","        ',',\n","        \"{:.2f}\".format(f1_kf),\n","    )\n","# %%\n"],"execution_count":8,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Final Result V1/Final Result V1/Resized_Histogram_Binning/6_kuditta_tattal_Histogram_Binning/*.mat\n","1 \t /content/drive/My Drive/Final Result V1/Final Result V1/Resized_Histogram_Binning/6_kuditta_tattal_Histogram_Binning/6_Kuditta_tattal_1_Dancer_1.mat\n","1 \t /content/drive/My Drive/Final Result V1/Final Result V1/Resized_Histogram_Binning/6_kuditta_tattal_Histogram_Binning/6_Kuditta_tattal_1_Dancer_2.mat\n","1 \t /content/drive/My Drive/Final Result V1/Final Result V1/Resized_Histogram_Binning/6_kuditta_tattal_Histogram_Binning/6_Kuditta_tattal_1_Dancer_3.mat\n","1 \t /content/drive/My Drive/Final Result V1/Final Result V1/Resized_Histogram_Binning/6_kuditta_tattal_Histogram_Binning/6_Kuditta_tattal_2_Dancer_1.mat\n","1 \t /content/drive/My Drive/Final Result V1/Final Result V1/Resized_Histogram_Binning/6_kuditta_tattal_Histogram_Binning/6_Kuditta_tattal_2_Dancer_2.mat\n","1 \t /content/drive/My Drive/Final Result V1/Final Result V1/Resized_Histogram_Binning/6_kuditta_tattal_Histogram_Binning/6_Kuditta_tattal_2_Dancer_3.mat\n","1 \t /content/drive/My Drive/Final Result V1/Final Result V1/Resized_Histogram_Binning/6_kuditta_tattal_Histogram_Binning/6_Kuditta_tattal_3_Dancer_1.mat\n","1 \t /content/drive/My Drive/Final Result V1/Final Result V1/Resized_Histogram_Binning/6_kuditta_tattal_Histogram_Binning/6_Kuditta_tattal_3_Dancer_2.mat\n","1 \t /content/drive/My Drive/Final Result V1/Final Result V1/Resized_Histogram_Binning/6_kuditta_tattal_Histogram_Binning/6_Kuditta_tattal_3_Dancer_3.mat\n","1 \t /content/drive/My Drive/Final Result V1/Final Result V1/Resized_Histogram_Binning/6_kuditta_tattal_Histogram_Binning/6_Kuditta_tattal_4_Dancer_1.mat\n","1 \t /content/drive/My Drive/Final Result V1/Final Result V1/Resized_Histogram_Binning/6_kuditta_tattal_Histogram_Binning/6_Kuditta_tattal_4_Dancer_2.mat\n","1 \t /content/drive/My Drive/Final Result V1/Final Result V1/Resized_Histogram_Binning/6_kuditta_tattal_Histogram_Binning/6_Kuditta_tattal_5_Dancer_1.mat\n","1 \t /content/drive/My Drive/Final Result V1/Final Result V1/Resized_Histogram_Binning/6_kuditta_tattal_Histogram_Binning/6_Kuditta_tattal_5_Dancer_2.mat\n","1 \t /content/drive/My Drive/Final Result V1/Final Result V1/Resized_Histogram_Binning/6_kuditta_tattal_Histogram_Binning/6_Kuditta_tattal_5_Dancer_3.mat\n","(17920, 9)\n","(17920,)\n","59.5703125 40.4296875\n","6_kuditta_tattal , 66.80 , 57.72 , 80.40 , 81.54 , 55.92 , 57.72 , 80.40 , 67.59 , 65.96\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bgGg4KWOuvEy","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}